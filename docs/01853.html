<html>
<head>
<title>The Truth Behind Liveplace's Photo-Realistic 3D World And OTOY's Rendering Engine • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">Liveplace照片般逼真的3D世界和OTOY渲染引擎背后的真相TechCrunch</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2008/08/20/the-truth-behind-liveplaces-photo-realistic-3d-world-and-otoys-rendering-engine/">https://web.archive.org/web/https://techcrunch.com/2008/08/20/the-truth-behind-liveplaces-photo-realistic-3d-world-and-otoys-rendering-engine/</a></blockquote><div><div class="article-content">
				<p id="speakable-summary" class="translated"><a href="https://web.archive.org/web/20230215155927/http://www.crunchbase.com/company/otoy"><img decoding="async" src="../Images/bdf37fe939b1c4b986106db2fb4793db.png" class="shot2" data-original-src="https://web.archive.org/web/20230215155927im_/https://techcrunch.com/wp-content/uploads/2008/08/otoylogo.png"/>T2】</a></p>
<p class="translated">上周，我们发布了一个<a href="https://web.archive.org/web/20230215155927/https://techcrunch.com/2008/08/11/liveplace-to-launch-photo-realistic-virtual-world-rendered-in-the-cloud/">视频</a>，展示了LivePlace，一个拥有难以置信的大量细节的3D世界。其背后令人印象深刻的技术被称为OTOY，这是一个流媒体平台，允许开发人员在“云中”生成电影质量的渲染，然后可以传输到性能更低的计算机甚至手机上。想了解更多关于OTOY的信息，请看我们的介绍文章<a href="https://web.archive.org/web/20230215155927/https://techcrunch.com/2008/07/09/otoy-developing-server-side-3d-rendering-technology/">这里</a>。</p>
<p class="translated">该视频在LivePlace.com向公众开放，旁边还有模糊的标题“直播还是虚拟直播？”但显然没人会发现它。我们发布帖子后不久，LivePlace就从服务器上删除了这段视频。拥有LivePlace的MySpace背后的企业家布拉德·格林斯潘(Brad Greenspan)说，该网站从来就不打算让公众看到，他解释说，这是为了内部模型、病毒视频和“类似于搞笑或死亡剧集的东西”这种解释并不适合我，但我们不太可能从格林斯潘那里得到任何更实质性的东西。</p>
<p class="translated">[http://blip.tv/play/AcjrSou8cA电视台]</p>
<p class="translated">那么，那个3D虚拟世界是什么——它是假的吗？</p>
<p class="translated">OTOY的创始人Jules Urbach解释说，虽然他不能评论Liveplace正在做什么(或者他们为什么发布视频)，但视频中运行在渲染引擎上的虚拟世界正在路上。他表示，这段视频并不代表他的系统的功能(自视频拍摄以来，该系统的功能实际上已经有所改善)，实际上只是由Liveplace拼接在一起的一些随机片段:</p>
<blockquote><p class="translated">“该材料中14分钟的实时渲染以240 kpbs的速度实时传输到Treo 700。这是在2007年3月捕获的，服务器运行ATI RX 1900 GPU。从那以后，技术有了很大的进步(就像我们现在运行的硬件一样)。我们从来没有打算向公众展示其中的任何部分，直到我们可以包括体素渲染和基于Lightstage的角色。我想任何喜欢他们所看到的东西的人，都会发现最终的项目更加令人印象深刻。</p>
<p class="translated">我们上个月在AMD的Ruby演示上所做工作的全部目标是展示从这一代GPU开始，离线和实时工作的质量是相同的。本月接下来的演示只是介绍Lightstage，以及它如何让角色(或任何CG对象)在实时环境中看起来100%真实。</p>
<p class="translated">直到今年晚些时候，在一个关于为OTOY开发的服务器端平台的进一步声明之后，这些技术将被应用到的虚拟世界才被讨论。</p>
<p class="translated">我们与编辑或泄露这个视频没有任何关系，除了OTOY技术之外，我们不能对任何事情发表评论，因为这个项目仍然在NDA之下。"</p></blockquote>
<p class="translated">除了在视频中看到的缺乏一致性，读者还担心它可能包含从其他艺术家那里盗版的材料。该视频以一个简短的汽车剪辑开始，显然是从艺术家的作品集中截取的，最初是几年前创作的。事实证明，这些片段是旧的，但朱尔斯·乌尔巴赫解释说，这位艺术家现在是OTOY团队的一员:</p>
<blockquote><p class="translated">“在过去的3年里，JJ一直与OTOY/JulesWorld在我们几乎所有的主要项目上进行合作(其中一些项目仍由NDA负责)。我非常自豪地把他视为一个伟大的朋友和合作伙伴。</p>
<p class="translated">JJ的工作室，BLR，总是在我们的客户让我们放上我们的标志的所有视频上得到适当的认可，无论是实时项目还是线性VFX工作。你可以在几周前Techcrunch上的实时变形金刚OTOY剪辑上看到BLR的标志(最初来自每日综艺)，你还会在11月份以我们的工作为主题的平面广告活动中再次看到它。</p>
<p class="translated">注意:你在BCN街头场景最开始看到的大众甲壳虫是JJ的第一个CG模型之一，是他的“宝贝”。它几乎出现在我们一起做的所有事情中——从我们为派拉蒙做的“大黄蜂”变形金刚广告，到我们最近为AMD做的Ruby voxel演示(你可以在街道的右侧找到它)。这也是上个月OTOY上TechCrunch文章中的<a href="https://web.archive.org/web/20230215155927/https://techcrunch.com/wp-content/uploads/2008/07/otoy15.png">图像之一</a>(在512 Mb R770上实时渲染，预体素渲染器)。</p></blockquote>
<p class="translated">那么底线是什么？LivePlace似乎与提供的视频或描述的城市没有任何关系，一开始就不应该发布这些镜头。<b>其背后令人印象深刻的OTOY技术是真实的，但我们必须等待，看看哪些产品将利用它</b>。</p>
<p class="translated">以下是朱尔斯提供的更多技术细节:<br/>T5</p>
<blockquote><p class="translated">–我们以多种方式处理体素数据，包括几何图(参见我们的Siggraph或冰岛演示，其中我们展示了这种方法应用于Ligthstage 5结构光数据，由安德鲁·琼斯ICT/Graphics实验室提供)</p>
<p class="translated">–来自BCN和红宝石城场景的数据集每个体素包含多达64个数据层，包括漫反射率、菲涅耳反射率值、辐照度数据、UV坐标(多达8组)、法线，对于静态场景，还包括来自多达252个均匀分布的视点的1-20次光反射的查找向量(需要注意的是，该数据始终是100%可选的，因为当体素接近且反射精度比速度更重要时，光线投射器可以在程序上做到这一点；但是，使用缓存的反射率数据，当场景没有变化时，您可能会看到场景以100-1000 fps的速度渲染。</p>
<p class="translated">–关于光线跟踪与光栅化的说明:使用GPU tessellator将Fincher的Bug Snuff演示中的树干放大到2800万个多边形，结果比渲染该对象的2800万个体素点云更快。因此，在大约1亿个多边形时，体素变得比光栅化快是有一个阈值的。至少在我们的引擎中，在R7xx GPUs上，使用1280×720的全精确光线投射。在这一点之下，使用GPU tessellator的传统光栅化对于单个视口来说似乎更快。</p>
<p class="translated">–在R770上，该引擎可以在约1/200秒内将一百万多边形网格转换为体素数据(在R600和8800 GTX上为60 fps)。这对于烘焙在GPU上一次性或偶尔生成的密集静态场景非常有用。这就是为什么有些OTOY演示要求GPU tessellator看起来正确。</p>
<p class="translated">–OTOY中的硬阴影是使用光栅化完成的，直到我们在5月份获得R770。现在，硬阴影，像反射，可以使用光线投射来计算，尽管阴影掩膜仍然非常有用，使用体素数据的光线投射仍然会给你带来走样。</p>
<p class="translated">–我们可以将光线投射器与程序生成的数据(柏林生成的地形或云、基于样条的对象等)一起使用。).在Jon Peddie的Siggraph活动中，我们展示了实时应用于Ruby street场景的变形。它是独立于分辨率的，就像一个闪光矢量对象，所以你可以无限接近它而没有阶梯效果，同样，阴影投射也会以同样的方式工作。</p>
<p class="translated">–体素数据被分组为大致相当于“三角形批次”(也可以索引到每个对象或每个材料组)。这使我们能够以与传统多边形网格非常相似的方式处理体素数据的子集。</p>
<p class="translated">–2007年3月“Treo”视频中的反射大约是我们现在在R770/R700上用于Ruby演示的光线投射的1/1000精确/快速。</p>
<p class="translated">–一个R770 GPU可以以“Treo”视频中显示的质量和大小渲染大约100多个视窗。当场景完全基于体素时，同步视口的数量不如所有视口的总渲染面积重要。</p>
<p class="translated">–服务器端渲染系统目前由使用8个R770 GPU(8 Gb VRAM，每箱1.5 Kw功率)的系统组成。</p>
</blockquote>

			</div>

			</div>    
</body>
</html>