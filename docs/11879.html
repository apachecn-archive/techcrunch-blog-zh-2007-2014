<html>
<head>
<title>Turn Pizza Boxes Into Computer Interfaces With "Invoked Computing" (Video) • TechCrunch</title>
<link href="../Styles/Style.css" type="text/css" rel="stylesheet"/>
</head>
<body>
<h1 class="translated">通过“调用计算”(视频)将披萨盒变成电脑界面</h1>
<blockquote>原文：<a href="https://web.archive.org/web/https://techcrunch.com/2011/11/21/turn-pizza-boxes-into-computer-interfaces-with-invoked-computing-video/">https://web.archive.org/web/https://techcrunch.com/2011/11/21/turn-pizza-boxes-into-computer-interfaces-with-invoked-computing-video/</a></blockquote><div><header class="article__header ">
	<p class="article__title-wrapper">
						</p><h1 class="article__title translated">通过“调用计算”(视频)，将披萨盒变成计算机界面</h1>
		

			
	
			
	</header>

			<div class="article-content">
				<p id="speakable-summary" class="translated">你想把你的披萨盒变成一个电脑界面，或者用香蕉代替手机吗？东京大学石川奥库实验室开发的概念调用计算通过“无处不在”的增强现实(视频和声音)使之成为可能。</p>
<p class="translated">这里的想法是将屏幕、键盘和其他元素投影到日常物品上，这样用户就不再需要特定的硬件了。</p>
<p class="translated">这些“增强”的对象可以是任何东西(如上面提到的披萨盒或电话)，并且可以被操纵，例如通过触摸披萨盒上的投影音量栏，并上下移动手指来调节现实世界中的音量。</p>
<p class="translated">【T2<img decoding="async" loading="lazy" class="aligncenter size-full wp-image-455800" title="Picture 3" src="../Images/16f26e654293e19123d0ebf382a05fbd.png" alt="" srcset="https://web.archive.org/web/20221207113741im_/https://techcrunch.com/wp-content/uploads/2011/11/picture-31.png 709w, https://web.archive.org/web/20221207113741im_/https://techcrunch.com/wp-content/uploads/2011/11/picture-31.png?resize=150,91 150w, https://web.archive.org/web/20221207113741im_/https://techcrunch.com/wp-content/uploads/2011/11/picture-31.png?resize=300,181 300w, https://web.archive.org/web/20221207113741im_/https://techcrunch.com/wp-content/uploads/2011/11/picture-31.png?resize=680,410 680w, https://web.archive.org/web/20221207113741im_/https://techcrunch.com/wp-content/uploads/2011/11/picture-31.png?resize=50,30 50w" sizes="(max-width: 640px) 100vw, 640px" data-original-src="https://web.archive.org/web/20221207113741im_/https://beta.techcrunch.com/wp-content/uploads/2011/11/picture-31.png"/></p>
<p class="translated">首席研究员亚历克西斯·泽罗格解释道:</p>
<blockquote><p class="translated">在这个项目中，我们探索了相反的场景:一种无处不在的智能，能够发现和实例化人类提出的启示<em>(作为模仿的动作和涉及物体和图画的场景)。Miming将促使无处不在的计算环境“浓缩”在真实对象上，通过常见的AR技术为其补充人工启示。举个例子:拿一根香蕉，把它靠近耳朵。这种姿态非常明显:隐藏在房间里的定向麦克风和参数扬声器将使香蕉在现场发挥真正手机的功能。(…)</em></p>
<p class="translated">为了“调用”一个应用程序，用户只需要<em>模拟一个特定的场景。</em>系统将尝试识别建议的启示，并通过AR技术实例化所表示的功能(另一个例子:为了调用笔记本电脑，用户可以拿起一个比萨饼盒子，打开它并在其表面上“粘贴”)。</p></blockquote>
<p class="translated"><a href="https://web.archive.org/web/20221207113741/http://www.diginfo.tv/2011/11/18/11-0232-d-en.php"> Diginfo TV </a>最近与Zerroug拍摄了一段视频，他在视频中解释了被调用的计算项目的最新状态:<br/>【YouTube = http://www . YouTube . com/watch？v = za 6 m2 fxpxzk&amp;w = 560&amp;h = 315】</p>
			</div>

			</div>    
</body>
</html>